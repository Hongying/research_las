{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from gensim import corpora, models, similarities\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl import Workbook\n",
    "from pprint import pprint\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#only need to do the following once\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Approach #1: topic modeling: each doc belongs to the topic with biggest probability. Output saved in OUTPUT_FILE.\n",
    "Approach #2: k-means based on topic distribution vector. Output saved in OUTPUT_FILE_K.\n",
    "\n",
    "@param\n",
    "NUM_OF_TOPICS: number of topics, used in Approach #1 topic modeling\n",
    "NUM_OF_CLUSTERS: number of clusters, used in Approach #2 k-means\n",
    "FILE: 'healthcare' or 'security'\n",
    "'''\n",
    "\n",
    "NUM_OF_TOPICS = 5\n",
    "NUM_OF_CLUSTERS = 5\n",
    "FILE = 'healthcare'\n",
    "INPUT_FILE = FILE + '.xlsx'\n",
    "OUTPUT_FILE = FILE + '_output_tm.xlsx'\n",
    "OUTPUT_FILE_K = FILE + '_output_kmeans.xlsx'\n",
    "\n",
    "# workbook and first sheet\n",
    "#@param: workbook name: healthcare.xlsx, or security.xlsx\n",
    "wb = load_workbook(INPUT_FILE)\n",
    "ws = wb[wb.get_sheet_names()[0]]\n",
    "# convert to list of tuples, each tuple is a row of cells in the excel\n",
    "ws_list = list(ws.rows)\n",
    "#ignore the first row (header)\n",
    "del ws_list[0]\n",
    "\n",
    "documents = []\n",
    "for row in ws_list:\n",
    "    tmp = \"\"\n",
    "    if(row[3].value != None):\n",
    "        tmp = row[3].value + \" \"\n",
    "    if(row[4].value != None):\n",
    "        tmp = tmp + row[4].value\n",
    "    documents.append(tmp)\n",
    "#print(documents[0:1])\n",
    "#print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['peopl', 'help', 'healthcar', 'improv', 'villag', 'doctor', 'provid', 'would', 'basic', 'sauri', 'educ', 'livelihood', 'could', 'clean', 'free', 'fund', 'hospit', 'health', 'awar', 'diseas']\n"
     ]
    }
   ],
   "source": [
    "# substitute with real document later, need to tokenize based on space, comma, period, question mark, and quotes.\n",
    "'''\n",
    "# test data\n",
    "documents = [\"Human machine. interface for lab abc computer applications\",\n",
    "             \"A survey of,user opinion of computer system response time\",\n",
    "             \"The EPS user interface management system\",\n",
    "             \"System and human system engineering testing of EPS\",              \n",
    "             \"Relation of user perceived response time to error measurement\",\n",
    "             \"The generation of random binary unordered trees\",\n",
    "             \"The intersection graph of paths in trees\",\n",
    "             \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "             \"Graph minors A survey\"]\n",
    "'''\n",
    "\n",
    "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# a tokenizer and stemmer, convert to lower case\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "    #return filtered_tokens\n",
    "\n",
    "# tokenize and stemming\n",
    "tokenized_text = [tokenize_and_stem(doc) for doc in documents]\n",
    "\n",
    "# remove stopwords; texts is a list of list of tokenized words\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "texts = [[word for word in text if word not in stopwords] \n",
    "         for text in tokenized_text]\n",
    "#pprint(texts)\n",
    "\n",
    "\n",
    "# create a Gensim dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "#print(dictionary)\n",
    "# remove extremes (similar to the min/max df step used when creating the tf-idf matrix)\n",
    "dictionary.filter_extremes(no_below=1, no_above=0.8)\n",
    "#print(dictionary)\n",
    "\n",
    "# show words and their ids\n",
    "#print(dictionary.token2id)\n",
    "\n",
    "# convert documents to vectors, e.g. [[(0, 1), (1, 1)],[...]]\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "#print(\"Corpus:\")\n",
    "#pprint(corpus)\n",
    "\n",
    "#Latent Dirichlet Allocation(LDA) topic modeling\n",
    "#lda = models.LdaModel(corpus, num_topics=NUM_OF_TOPICS,id2word=dictionary,update_every=5, chunksize=10000, passes=5)\n",
    "#lda.show_topics()\n",
    "\n",
    "lda = models.LdaModel(corpus, num_topics = NUM_OF_TOPICS,id2word = dictionary)\n",
    "topics_matrix = lda.show_topics(formatted=False, num_words=20)\n",
    "#print(topics_matrix[0])\n",
    "\n",
    "#topics is a list of list of topics, e.g.: [['peopl', 'healthcar', 'help'],...]\n",
    "topics = []\n",
    "for topic in topics_matrix:\n",
    "    topics.append([words for (words,freq) in topic[1]])\n",
    "print(topics[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 0.011346224051494751), (1, 0.011313701854032174), (2, 0.95463117822926513), (3, 0.011356962119535859), (4, 0.011351933745672005)], [(0, 0.015592579010758247), (1, 0.015649729840445183), (2, 0.93760785978776673), (3, 0.01565136467616204), (4, 0.015498466684867797)], [(0, 0.18733965528971688), (4, 0.78471461274136123)], [(0, 0.012766312963106706), (1, 0.012630910931382544), (2, 0.012678852502441945), (3, 0.94919250030058733), (4, 0.012731423302481396)], [(4, 0.98227955820529067)]]\n"
     ]
    }
   ],
   "source": [
    "# clustering\n",
    "doc_lda = []\n",
    "for doc_bow in corpus:\n",
    "    #doc_lda.append(lda[doc_bow]) #works too.\n",
    "    doc_lda.append(lda.get_document_topics(doc_bow))\n",
    "print(doc_lda[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 7, 17, 22, 27, 38, 46, 48, 58, 63, 64, 65, 68, 71, 72, 81, 82, 84, 97, 100, 101, 102, 103, 104, 105, 106, 109, 115, 119, 124, 126, 128, 129, 131, 135, 143, 151, 158, 163, 165, 168, 169, 176, 180, 181, 186, 189, 199, 205], [15, 23, 24, 29, 35, 36, 37, 42, 47, 54, 74, 80, 99, 112, 120, 122, 130, 134, 138, 141, 145, 148, 149, 153, 161, 192, 200, 209], [0, 1, 11, 13, 16, 18, 26, 28, 31, 33, 40, 51, 66, 70, 78, 89, 92, 96, 111, 113, 121, 123, 127, 133, 137, 144, 147, 150, 159, 171, 172, 173, 182, 190, 194, 198, 201, 202, 204, 207], [3, 6, 8, 10, 12, 14, 19, 20, 21, 25, 41, 44, 45, 49, 50, 53, 55, 56, 60, 61, 62, 67, 73, 75, 83, 85, 87, 88, 90, 94, 95, 98, 107, 132, 136, 139, 142, 146, 152, 156, 157, 160, 162, 164, 166, 167, 170, 175, 177, 179, 183, 184, 193, 195, 196, 203, 208], [2, 4, 9, 30, 32, 34, 39, 43, 52, 57, 59, 69, 76, 77, 79, 86, 91, 93, 108, 110, 114, 116, 117, 118, 125, 140, 154, 155, 174, 178, 185, 187, 188, 191, 197, 206]]\n"
     ]
    }
   ],
   "source": [
    "# Approach #1: cluster a document to the cluster with the highest topic score\n",
    "clusters = [[] for i in range(0,NUM_OF_TOPICS)] # each element is a cluster, [[doc_id, ...]...]\n",
    "doc_topic_prob = [] #[[topic_id, prob]...], each element corresponds to a doc\n",
    "\n",
    "for topic_distr in doc_lda:\n",
    "    score = max(topic_distr,key=lambda x:x[1])\n",
    "    doc_topic_prob.append(score)\n",
    "#print(doc_topic_prob[0:2])\n",
    "\n",
    "for (index,item) in enumerate(doc_topic_prob):\n",
    "    clusters[item[0]].append(index)\n",
    "print(clusters[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#put original data values to a list, prepare for write\n",
    "#print(ws_list[0:2])\n",
    "ws_list_o = []\n",
    "for row in ws_list:\n",
    "    row_o = []\n",
    "    for cell in row:\n",
    "        row_o.append(cell.value)\n",
    "    ws_list_o.append(row_o)\n",
    "#print(ws_list_o[0:2])\n",
    "\n",
    "# Approach #1: write clusters in the OUTPUT_FILE. Clusters seperated by an empty line.\n",
    "\n",
    "def write_output(clusters, output_file, topics=None):\n",
    "    wb_o = Workbook()\n",
    "    ws_o = wb_o.active  #the first spreadsheet\n",
    "    for index, cluster in enumerate(clusters):\n",
    "        if topics != None:\n",
    "            ws_o.append(topics[index])\n",
    "        for document_id in cluster:\n",
    "            ws_o.append(ws_list_o[document_id])\n",
    "        ws_o.append([])\n",
    "    wb_o.save(output_file)\n",
    "    \n",
    "write_output(clusters, OUTPUT_FILE, topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 3, 2, 3, 1, 2, 1, 2, 3, 2, 0, 2, 0, 2, 4, 0, 1, 0, 2, 2, 2, 1, 4, 4, 2, 0, 1, 0, 4, 3, 0, 3, 0, 3, 4, 4, 4, 1, 3, 0, 2, 4, 3, 2, 2, 1, 4, 1, 2, 2, 0, 3, 2, 4, 2, 2, 3, 1, 3, 2, 2, 2, 1, 1, 1, 0, 2, 1, 3, 0, 1, 1, 2, 4, 2, 3, 3, 0, 3, 4, 1, 1, 2, 1, 2, 3, 2, 2, 0, 2, 3, 0, 3, 2, 2, 0, 1, 2, 4, 1, 1, 3, 1, 1, 1, 1, 2, 3, 1, 3, 0, 4, 0, 3, 1, 3, 3, 3, 1, 4, 0, 4, 0, 1, 3, 1, 0, 1, 1, 4, 1, 2, 0, 4, 1, 2, 0, 4, 2, 3, 4, 2, 1, 0, 4, 2, 0, 4, 4, 0, 1, 2, 4, 3, 3, 2, 2, 1, 0, 2, 4, 2, 1, 2, 1, 2, 2, 1, 1, 2, 0, 0, 0, 3, 2, 1, 2, 3, 2, 1, 1, 0, 2, 2, 3, 1, 3, 3, 1, 0, 3, 4, 2, 0, 2, 2, 3, 0, 1, 4, 0, 0, 2, 0, 1, 3, 0, 2, 4]\n",
      "[[0, 1, 11, 13, 16, 18, 26, 28, 31, 33, 40, 51, 66, 70, 78, 89, 92, 96, 111, 113, 121, 123, 127, 133, 137, 144, 147, 150, 159, 171, 172, 173, 182, 190, 194, 198, 201, 202, 204, 207], [5, 7, 17, 22, 27, 38, 46, 48, 58, 63, 64, 65, 68, 71, 72, 81, 82, 84, 97, 100, 101, 103, 104, 105, 106, 109, 115, 119, 124, 126, 128, 129, 131, 135, 143, 151, 158, 163, 165, 168, 169, 176, 180, 181, 186, 189, 199, 205], [3, 6, 8, 10, 12, 14, 19, 20, 21, 25, 41, 44, 45, 49, 50, 53, 55, 56, 60, 61, 62, 67, 73, 75, 83, 85, 87, 88, 90, 94, 95, 98, 107, 132, 136, 139, 142, 146, 152, 156, 157, 160, 162, 164, 166, 167, 170, 175, 177, 179, 183, 184, 193, 195, 196, 203, 208], [2, 4, 9, 30, 32, 34, 39, 43, 52, 57, 59, 69, 76, 77, 79, 86, 91, 93, 102, 108, 110, 114, 116, 117, 118, 125, 140, 154, 155, 174, 178, 185, 187, 188, 191, 197, 206], [15, 23, 24, 29, 35, 36, 37, 42, 47, 54, 74, 80, 99, 112, 120, 122, 130, 134, 138, 141, 145, 148, 149, 153, 161, 192, 200, 209]]\n"
     ]
    }
   ],
   "source": [
    "# Approach #2: k-means clustering, using topic distribution vector as input vector\n",
    "km_input = []\n",
    "\n",
    "for doc in doc_lda:\n",
    "    vec = []\n",
    "    # some len(doc) < NUM_OF_TOPICS, need to patch them with 0s. -> needed in km.fix() function.\n",
    "    if len(doc) == NUM_OF_TOPICS:\n",
    "        vec = [y for (x,y) in doc]\n",
    "    else:        \n",
    "        for i in range(0, NUM_OF_TOPICS):\n",
    "            tmp = [x for x, y in enumerate(doc) if y[0] == i]\n",
    "            if tmp == []:\n",
    "                vec.append(0)\n",
    "            else:\n",
    "                vec.append(doc[tmp[0]][1])\n",
    "    km_input.append(vec)\n",
    "#print(km_input[0:5])\n",
    "\n",
    "#to check whether all elements in km_input has the same length\n",
    "#print(np.unique(list(map(len, km_input))))\n",
    "X = np.array(km_input)\n",
    "km = KMeans(n_clusters=NUM_OF_CLUSTERS, random_state=0).fit(X)\n",
    "\n",
    "kmeans_labels = km.labels_.tolist()\n",
    "print(kmeans_labels)\n",
    "\n",
    "clusters_kmeans = [[] for i in range(0, NUM_OF_CLUSTERS)] # each element is a cluster, [[doc_id, ...]...]\n",
    "for index, cluster in enumerate(kmeans_labels):\n",
    "    clusters_kmeans[cluster].append(index)\n",
    "print(clusters_kmeans[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach #2: write output in OUTPUT_FILE_K.\n",
    "write_output(clusters_kmeans, OUTPUT_FILE_K)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
